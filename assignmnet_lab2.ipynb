{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ecbb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with imports - ask ChatGPT to explain any package that you don't know\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db341174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always remember to do this!\n",
    "load_dotenv(override=True) # This is to bring in the environment variables and override isset to True to override any existing environment variables taking priorities\n",
    "# Print the key prefixes to help with any debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9c301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb2900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. \"\n",
    "request += \"Answer only with the question, no explanation.\"\n",
    "messages = [{\"role\": \"user\", \"content\": request}]\n",
    "\n",
    "print(\"request:\", request)\n",
    "messages\n",
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    ")\n",
    "question = response.choices[0].message.content\n",
    "print(question)\n",
    "competitors=[] # creating a new empty list\n",
    "answers=[]\n",
    "messages = [{\"role\":\"user\", \"content\":question}]\n",
    "\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1940c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"gpt-4o-mini\"\n",
    "\n",
    "response = openai.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505df240",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Anthropic has a slightly different API, and Max Tokens is required\n",
    "model_name = \"claude-3-7-sonnet-latest\"\n",
    "\n",
    "claude = Anthropic()\n",
    "response = claude.messages.create(model=model_name, messages=messages, max_tokens=1000)\n",
    "answer = response.content[0].text\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4527139f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_name = \"gemini-2.0-flash\"\n",
    "\n",
    "response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "# print(response)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706a4c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "model_name = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "response = groq.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c761db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For the next cell, we will use Ollama\n",
    "\n",
    "ollama = OpenAI(base_url=\"http://localhost:11434/v1\", api_key='ollama')\n",
    "model_name = \"llama3.2\"\n",
    "\n",
    "response = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2965b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So where are we?\n",
    "\n",
    "print(competitors)\n",
    "print(answers)\n",
    "for competitor, answer in zip(competitors, answers):\n",
    "    print(f\"Competitor: {competitor}\\n\\n{answer}\")\n",
    "# It's nice to know how to use \"zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5f057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's bring this together - note the use of \"enumerate\"\n",
    "\n",
    "together = \"\" # empty string\n",
    "for index, answer in enumerate(answers):\n",
    "    together += f\"# Response from competitor {index+1}\\n\\n\"\n",
    "    together += answer + \"\\n\\n\"\n",
    "print(together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00a88f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evauator - optimizer - Adding another agentic pattern - Self-loop design pattern - Where there's a feedback loop - the generated output goes as an input to the evauator and either gets rejected or approve by it. If rejected, the response generated next will be an improved one.\n",
    "\n",
    "# Critic pattern: each competitor answer is critiqued by another agent\n",
    "\n",
    "# critiqued_answers = []\n",
    "# for competitor, answer in zip(competitors, answers):\n",
    "#     critique_prompt = f\"\"\"You are a critic. Please evaluate the following response from {competitor} for clarity, factual correctness, and depth.\n",
    "#     Suggest improvements, but do not rewrite the full response.\n",
    "    \n",
    "#     Response:\n",
    "#     {answer}\n",
    "#     \"\"\"\n",
    "\n",
    "#     critique_messages = [{\"role\": \"user\", \"content\": critique_prompt}]\n",
    "\n",
    "#     response = openai.chat.completions.create(\n",
    "#         model=\"gpt-4o-mini\",\n",
    "#         messages=critique_messages,\n",
    "#         temperature=0.2,\n",
    "#         seed=42\n",
    "#     )\n",
    "\n",
    "#     critique = response.choices[0].message.content\n",
    "#     critiqued_answers.append(f\"{answer}\\n\\nCritique: {critique}\")\n",
    "#     display(Markdown(critique))\n",
    "#####################################################################\n",
    "\n",
    "# --- Critic + Reflection (sef-refine) block\n",
    "\n",
    "# keep a copy of the original answers (so we can critiqu and then refine)\n",
    "original_answers = answers.copy()\n",
    "\n",
    "critiqued_answers = []\n",
    "critiques = []\n",
    "\n",
    "# 1) Critic step: produce a critique for each answer\n",
    "for competitor, answer in zip(competitors, original_answers):\n",
    "    critique_prompt = f\"\"\"You are a critic. Please evaluate the folowing response from {competitor} for:\n",
    "- Clarity\n",
    "- Factual correctness\n",
    "- Depth\n",
    "\n",
    "Suggets concrete improvements, but do not rewrite the full response.\n",
    "\n",
    "Response:\n",
    "{answer}\n",
    "\"\"\"\n",
    "\n",
    "    critique_messages = [{\"role\": \"user\", \"content\": critique_prompt}]\n",
    "    try:\n",
    "        resp = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=critique_messages,\n",
    "            temperature=0.2,\n",
    "            seed=42 # Use an integer seed for reproducibility\n",
    "        )\n",
    "        critique = resp.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        critique = f\"ERROR (critique failed): {e}\"\n",
    "\n",
    "    critiques.append(critique)\n",
    "    critiqued_answers.append(f\"{answer}\\n\\n**Critique:** {critique}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17fa4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(critiqued_answers)\n",
    "\n",
    "# show all critiques together (neater than showing only the last)\n",
    "display(Markdown(\"\\n\\n---\\n\\n\".join(f\"### {c}\\n\\n{cr}\" for c, cr in zip(competitors, critiques))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18f1157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Reflection / Self-refine step: improve each original answer using the critique\n",
    "refined_answers = []\n",
    "for competitor, answer, critique in zip(competitors, answers, critiques):\n",
    "    refine_prompt = f\"\"\"You wrote the response below. Improve it uisng the critique.\n",
    "Keep the same stance, but fix clarity, correctness, and depth. Be concise.\n",
    "\n",
    "Original response:\n",
    "{answer}\n",
    "\n",
    "Critique:\n",
    "{critique}\n",
    "\n",
    "Revised response (final):\n",
    "\"\"\"\n",
    "    try:\n",
    "        response2 = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": refine_prompt}],\n",
    "            temperature=0.2,\n",
    "            seed=42\n",
    "        )\n",
    "        refined = response2.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        # fallback - if refinement fails, keep original answer\n",
    "        refined = answer\n",
    "        refined += f\"\\n\\n[Refinement failed: {e}]\"\n",
    "\n",
    "    refined_answers.append(refined)\n",
    "    # show each refined answer right away for transparency\n",
    "    display(Markdown(f\"### Refined answer for {competitor}\\n\\n{refined}\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3859c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# The API we know well\n",
    "\n",
    "# model_name = \"gpt-4o-mini\"\n",
    "\n",
    "# response = openai.chat.completions.create(model=model_name, messages=messages)\n",
    "# answer = response.choices[0].message.content\n",
    "\n",
    "# display(Markdown(answer))\n",
    "# competitors.append(model_name)\n",
    "# answers.append(answer)\n",
    "\n",
    "\n",
    "# Anthropic has a slightly different API, and Max Tokens is required\n",
    "\n",
    "# model_name = \"claude-3-7-sonnet-latest\"\n",
    "\n",
    "# claude = Anthropic()\n",
    "# response = claude.messages.create(model=model_name, messages=messages, max_tokens=1000)\n",
    "# answer = response.content[0].text\n",
    "\n",
    "# display(Markdown(answer))\n",
    "# competitors.append(model_name)\n",
    "# answers.append(answer)\n",
    "\n",
    "\n",
    "\n",
    "# gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "# model_name = \"gemini-2.0-flash\"\n",
    "\n",
    "# response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "# answer = response.choices[0].message.content\n",
    "\n",
    "# display(Markdown(answer))\n",
    "# competitors.append(model_name)\n",
    "# answers.append(answer)\n",
    "\n",
    "\n",
    "# deepseek = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com/v1\")\n",
    "# model_name = \"deepseek-chat\"\n",
    "\n",
    "# response = deepseek.chat.completions.create(model=model_name, messages=messages)\n",
    "# answer = response.choices[0].message.content\n",
    "\n",
    "# display(Markdown(answer))\n",
    "# competitors.append(model_name)\n",
    "# answers.append(answer)\n",
    "\n",
    "# deepseek = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com/v1\")\n",
    "# model_name = \"deepseek-chat\"\n",
    "\n",
    "# response = deepseek.chat.completions.create(model=model_name, messages=messages)\n",
    "# answer = response.choices[0].message.content\n",
    "\n",
    "# display(Markdown(answer))\n",
    "# competitors.append(model_name)\n",
    "# answers.append(answer)\n",
    "# groq = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "# model_name = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "# response = groq.chat.completions.create(model=model_name, messages=messages)\n",
    "# answer = response.choices[0].message.content\n",
    "\n",
    "# display(Markdown(answer))\n",
    "# competitors.append(model_name)\n",
    "# answers.append(answer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for competitor, answer in zip(competitors, answers):\n",
    "#     print(f\"Competitor: {competitor}\\n\\n{answer}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94f8e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answers = critiqued_answers\n",
    "# 3) Use the refined answers for judging\n",
    "answers = refined_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e07318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's bring this together - note the use of \"enumerate\"\n",
    "\n",
    "together = \"\" # empty string\n",
    "for index, answer in enumerate(answers):\n",
    "    together += f\"# Response from competitor {index+1}\\n\\n\"\n",
    "    together += answer + \"\\n\\n\"\n",
    "print(together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75efd8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# judge = f\"\"\"You are judging a competition between {len(competitors)} competitors.\n",
    "# Each model has been given this question:\n",
    "\n",
    "# {question}\n",
    "\n",
    "# Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
    "# Respond with JSON, and only JSON, with the following format:\n",
    "\n",
    "# {{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}}\n",
    "\n",
    "# Here are the responses from each competitor:\n",
    "\n",
    "# {together}\n",
    "\n",
    "# Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\"\"\"\n",
    "\n",
    "def run_judging(stage_name, answers, competitors, question):\n",
    "    together = \"\"\n",
    "    for index, answer in enumerate(answers):\n",
    "        together += f\"# Response from competitor {index+1}\\n\\n{answer}\\n\\n\"\n",
    "\n",
    "    judge_prompt = f\"\"\"You are judging a competition between {len(competitors)} competitors.\n",
    "Each model has been given this question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
    "Respond with JSON, and only JSON, with the following format:\n",
    "\n",
    "{{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}}\n",
    "\n",
    "Here are the responses from each competitor (stage: {stage_name}):\n",
    "\n",
    "{together}\n",
    "\n",
    "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\n",
    "\"\"\"\n",
    "    judge_messages = [{\"role\": \"user\", \"content\": judge_prompt}]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"o3-mini\",\n",
    "        messages=judge_messages\n",
    "    )\n",
    "    results = response.choices[0].message.content\n",
    "    print(f\"\\n===== {stage_name} Judging Results =====\")\n",
    "    print(results)\n",
    "\n",
    "    results_dict = json.loads(results)\n",
    "    ranks = results_dict[\"results\"]\n",
    "    for index, result in enumerate(ranks):\n",
    "        competitor = competitors[int(result)-1]\n",
    "        print(f\"Rank {index+1}: {competitor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4150c6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# print(judge)\n",
    "# judge_messages =  [{\"role\": \"user\", \"content\": judge}]\n",
    "# # Judgement time!\n",
    "\n",
    "# # openai = OpenAI()\n",
    "# # response = openai.chat.completions.create(\n",
    "# #     model=\"o3-mini\",\n",
    "# #     messages=judge_messages,\n",
    "# # )\n",
    "# # results = response.choices[0].message.content\n",
    "# # print(results)\n",
    "\n",
    "# openai = OpenAI()\n",
    "# response = openai.chat.completions.create(\n",
    "#     model=\"o3-mini\",\n",
    "#     messages=judge_messages\n",
    "# )\n",
    "# results = response.choices[0].message.content\n",
    "# print(results)\n",
    "# # # OK let's turn this into results!\n",
    "\n",
    "# # results_dict = json.loads(results)\n",
    "# # ranks = results_dict[\"results\"]\n",
    "# # for index, result in enumerate(ranks):\n",
    "# #     competitor = competitors[int(result)-1]\n",
    "# #     print(f\"Rank {index+1}: {competitor}\")\n",
    "\n",
    "# results_dict = json.loads(results)\n",
    "# ranks = results_dict[\"results\"]\n",
    "# for index, result in enumerate(ranks):\n",
    "#     competitor = competitors[int(result)-1]\n",
    "#     print(f\"Rank {index +1}: {competitor}\")\n",
    "\n",
    "# # The project above demonstrates multi -agent evaluation, wherein several agents are being asked to perform the same exercise and then their responses are collected.\n",
    "# # Then there's one more: Judge / refree - a separate LLM (\"o3-mini\") acts like a judge to compare and rank the responses.\n",
    "# # And the Orchestration design pattern - The code coordinates different APIs (OpenAI, Gemini, Ollama, Anthropic, Deepseek, Groq)\n",
    "\n",
    "# # Evauator - optimizer - Adding another agentic pattern - Self-loop design pattern - Where there's a feedback loop - the generated output goes as an input to the evauator and either gets rejected or approve by it. If rejected, the response generated next will be an improved one.\n",
    "\n",
    "\n",
    "# Run 3 rounds of judging\n",
    "run_judging(\"Original\", original_answers, competitors, question)\n",
    "run_judging(\"Critiqued\", critiqued_answers, competitors, question)\n",
    "run_judging(\"Refined\", refined_answers, competitors, question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8a60ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
